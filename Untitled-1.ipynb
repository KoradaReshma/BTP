{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CTU13_Attack_Traffic.csv: (38898, 59)\n",
      "\n",
      "Final dataset shape: (38898, 59)\n",
      "Loaded CTU13_Normal_Traffic.csv: (53314, 59)\n",
      "\n",
      "Final dataset shape: (53314, 59)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_dataset_from_folder(folder_path):\n",
    "    \"\"\"Load all CSV files from folder and combine into single DataFrame\"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded {file}: {df.shape}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No CSV files found in the folder\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nFinal dataset shape: {combined_df.shape}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Usage - replace with your actual folder path\n",
    "attack_path = \"c:\\\\Users\\\\Korad\\\\Downloads\\\\ctu_13_testing\\\\attack\"\n",
    "normal_path = \"C:\\\\Users\\\\korad\\\\Downloads\\\\ctu_13_testing\\\\normal\"\n",
    "df_attack = load_dataset_from_folder(attack_path)\n",
    "df_normal = load_dataset_from_folder(normal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b5d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_feature_set(df: pd.DataFrame, set_type: str) -> pd.DataFrame:\n",
    "    if set_type.upper() == \"A\":\n",
    "        selected_features = [f for f in set_a_features if f in df.columns]\n",
    "    elif set_type.upper() == \"B\":\n",
    "        selected_features = [f for f in set_b_features if f in df.columns]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid set_type. Use 'A' for LSTM/CNN or 'B' for Random Forest.\")\n",
    "    \n",
    "    return df[selected_features].copy()\n",
    "\n",
    "\n",
    "def extract_row_features(row: pd.Series, set_type: str) -> pd.DataFrame:\n",
    "    # Convert row to DataFrame\n",
    "    row_df = pd.DataFrame([row])\n",
    "    return get_feature_set(row_df, set_type)\n",
    "\n",
    "# ----------------------------\n",
    "# Example: file-level usage\n",
    "# ----------------------------\n",
    "\n",
    "def process_file(file_path: str, set_type: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    filtered_df = get_feature_set(df, set_type)\n",
    "    return filtered_df\n",
    "def all_feats(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "800c6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_features(df: pd.DataFrame, required_features: list) -> pd.DataFrame:\n",
    "    for feature in required_features:\n",
    "        if feature not in df.columns:\n",
    "            df[feature] = 0\n",
    "    return df\n",
    "def conversion_list_to_df(df_attack_layer_2,available_set_a):\n",
    "    df_attack_layer_2 = pd.DataFrame(df_attack_layer_2, columns=list(available_set_a) + [\"label\"])\n",
    "    return df_attack_layer_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d040cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal=all_feats(\"C:\\\\Users\\\\korad\\\\Downloads\\\\benign.csv\")\n",
    "df_normal=ensure_features(df_normal,['A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27979b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df_attack_layer_2 = []\n",
    "folder_path =\"C:\\\\Users\\\\korad\\\\Downloads\\\\attacks_file\\\\csv_output\"\n",
    "\n",
    "for file_path in os.listdir(folder_path):\n",
    "    full_path = os.path.join(folder_path, file_path)\n",
    "    file_name = os.path.splitext(file_path)[0]  # get file name without extension\n",
    "    df = process_file(full_path, 'A')  # process the file\n",
    "    if file_name == \"neris\":\n",
    "        df[\"label\"] = 1\n",
    "    elif file_name in [\"rbot_1\", \"rbot_2\"]:\n",
    "        df[\"label\"] = 2\n",
    "    else:\n",
    "        df[\"label\"] = 3\n",
    "    \n",
    "    df_attack_layer_2.append(df)\n",
    "df_attack_layer_2 = pd.concat(df_attack_layer_2, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f0013e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack_layer_3=[]\n",
    "for file_path in os.listdir(folder_path):\n",
    "    full_path = os.path.join(folder_path, file_path)\n",
    "    file_name = os.path.splitext(file_path)[0]  # get file name without extension\n",
    "    df = process_file(full_path, 'B')  # process the file\n",
    "    if file_name == \"menti\":\n",
    "        df[\"label\"] = 1\n",
    "    elif file_name==\"murlo\":\n",
    "        df[\"label\"] = 2\n",
    "    elif file_name==\"nsisay\":\n",
    "        df[\"label\"] = 3\n",
    "    elif file_name==\"virut\":\n",
    "        df[\"label\"] = 4\n",
    "    \n",
    "    df_attack_layer_3.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ac5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack_layer_4_menti=[]\n",
    "df_attack_layer_4_murlo=[]\n",
    "df_attack_layer_4_nsisay=[]\n",
    "df_attack_layer_4_virut=[]\n",
    "\n",
    "for file_path in os.listdir(folder_path):\n",
    "    full_path = os.path.join(folder_path, file_path)\n",
    "    file_name = os.path.splitext(file_path)[0]  # get file name without extension\n",
    "    df = all_feats(full_path) \n",
    "    df=ensure_features(df,['A','B'])\n",
    "    if file_name == \"menti\":\n",
    "        df_attack_layer_4_menti.append(df)\n",
    "    elif file_name==\"murlo\":\n",
    "        df_attack_layer_4_murlo.append(df)\n",
    "    elif file_name==\"nsisay\":\n",
    "        df_attack_layer_4_nsisay.append(df)\n",
    "    elif file_name==\"murlo\":\n",
    "        df_attack_layer_4_virut.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae_simple(input_dim, latent_dim=8):\n",
    "    \"\"\"Simple VAE without complex tensor operations\"\"\"\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(32, activation='relu')(inputs)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Latent space\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    # Simple sampling using Keras ops only\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(16, activation='relu')(z)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    vae = Model(inputs, outputs)\n",
    "    vae.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return vae\n",
    "\n",
    "# Train function\n",
    "def train_layer1_vae(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    input_dim = numeric_df.shape[1]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(numeric_df)\n",
    "    \n",
    "    vae = build_vae_simple(input_dim)\n",
    "    \n",
    "    print(\"Training VAE Layer 1...\")\n",
    "    history = vae.fit(X_scaled, X_scaled, \n",
    "                     epochs=50, \n",
    "                     batch_size=32, \n",
    "                     validation_split=0.2,\n",
    "                     verbose=1)\n",
    "    \n",
    "    return vae, scaler\n",
    "\n",
    "# Train\n",
    "vae_layer1, scaler_layer1 = train_layer1_vae(df_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NEEDS TO BE VERIFIED\n",
    "def detect_attacks_layer1(test_df, vae_model, scaler, threshold=0.1):\n",
    "    \"\"\"Run trained VAE on test data and detect attacks based on threshold\"\"\"\n",
    "    \n",
    "    # Select numeric features and scale\n",
    "    numeric_test = test_df.select_dtypes(include=[np.number])\n",
    "    X_test_scaled = scaler.transform(numeric_test)\n",
    "    \n",
    "    # Get reconstructions\n",
    "    reconstructions = vae_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate MAE (Mean Absolute Error)\n",
    "    mae = np.mean(np.abs(X_test_scaled - reconstructions), axis=1)\n",
    "    \n",
    "    # Classify as attack if MAE > threshold\n",
    "    attack_mask = mae > threshold\n",
    "    attack_df = test_df[attack_mask].copy()\n",
    "    \n",
    "    print(f\"Detected {len(attack_df)} attacks out of {len(test_df)} samples\")\n",
    "    print(f\"Attack rate: {len(attack_df)/len(test_df):.2%}\")\n",
    "    \n",
    "    return attack_df, mae\n",
    "\n",
    "# Usage\n",
    "attack_df_layer1, mae_scores = detect_attacks_layer1(df_attack, vae_layer1, scaler_layer1, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c22a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_a_features = [\n",
    "    # Flow time & rate features\n",
    "    'Flow Duration', 'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Fwd Pkts/s', 'Bwd Pkts/s',\n",
    "\n",
    "  # Inter-arrival times\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
    "    'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "    'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Bwd IAT Tot', 'Fwd IAT Tot',\n",
    "\n",
    "    # Packet size stats (for temporal variance)\n",
    "    'Pkt Len Mean', 'Pkt Len Std', 'Pkt Size Avg',\n",
    "\n",
    "    # Active/Idle patterns (C&C periodic behaviour)\n",
    "    'Active Mean', 'Idle Mean',\n",
    "\n",
    "    # Subflow dynamics (important for Neris/Rbot)\n",
    "    'Subflow Fwd Byts', 'Subflow Bwd Byts'\n",
    "]\n",
    "set_b_features = [\n",
    "    # Protocol and ports\n",
    "    'Protocol',\n",
    "\n",
    "    # Flag-based patterns\n",
    "    'SYN Flag Cnt', 'ACK Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'FIN Flag Cnt',\n",
    "\n",
    "    # Flow-level packet stats\n",
    "    'Flow Duration', 'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Max', 'Pkt Len Min',\n",
    "\n",
    "    # Header and ratio cues\n",
    "    'Fwd Header Len', 'Bwd Header Len', 'Down/Up Ratio'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13393bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "available_set_a = set_a_features\n",
    "def build_lstm_cnn_model(input_shape):\n",
    "    \"\"\"Build combined LSTM + CNN model for Layer 2 - Fixed version\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM branch for temporal patterns\n",
    "    lstm_branch = layers.LSTM(64, return_sequences=False)(inputs)  # Remove return_sequences\n",
    "    \n",
    "    # CNN branch for local patterns - need to handle 1D conv properly\n",
    "    # For 1D CNN, input should be (batch, steps, features)\n",
    "    cnn_branch = layers.Conv1D(32, kernel_size=1, activation='relu')(inputs)  # kernel_size=1 for single step\n",
    "    cnn_branch = layers.Conv1D(64, kernel_size=1, activation='relu')(cnn_branch)\n",
    "    cnn_branch = layers.GlobalMaxPooling1D()(cnn_branch)\n",
    "    \n",
    "    # Combine both branches\n",
    "    combined = layers.concatenate([lstm_branch, cnn_branch])\n",
    "    combined = layers.Dense(32, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.3)(combined)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_layer2_data(attack_df, temporal_features):\n",
    "    \"\"\"Prepare data for Layer 2 LSTM+CNN model\"\"\"\n",
    "    # Select temporal features\n",
    "    X = attack_df[temporal_features].values\n",
    "    \n",
    "    # Reshape for LSTM/CNN (samples, timesteps=1, features)\n",
    "    X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    # Create binary labels (1 for Neris/Rbot, 0 for others)\n",
    "    # You need to adjust this based on your actual label column\n",
    "    y = attack_df['Label']\n",
    "    return X_reshaped, y\n",
    "\n",
    "# Train Layer 2\n",
    "def train_layer2_model(df_attack_layer_2):\n",
    "    \"\"\"Train LSTM+CNN model on temporal features\"\"\"\n",
    "    # Separate features and label\n",
    "    X = df_attack_layer_2.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "    y = df_attack_layer_2[\"label\"].values.astype(np.int32)\n",
    "\n",
    "    # Reshape for LSTM/CNN (samples, timesteps=1, features)\n",
    "    X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # Build model\n",
    "    input_shape = (1, X.shape[1])   # (timesteps=1, features)\n",
    "    model = build_lstm_cnn_model(input_shape)\n",
    "\n",
    "    print(\"Training Layer 2 (LSTM+CNN)...\")\n",
    "    history = model.fit(\n",
    "        X_reshaped, y,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train Layer 2 on attacks from Layer 1\n",
    "# df_attack_layer_2=conversion_list_to_df(df_attack_layer_2,set_a_features)\n",
    "layer2_model = train_layer2_model(df_attack_layer_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6a05a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1160/1160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step\n",
      "Layer 2 detected 0 attacks with label 2\n"
     ]
    }
   ],
   "source": [
    "def predict_layer2(model, test_data, set_a_features):\n",
    "    \"\"\"Predict using Layer 2 model and filter samples with label prediction 2\"\"\"\n",
    "    # Select Set A features from test data\n",
    "    available_features = [f for f in set_a_features if f in test_data.columns]\n",
    "    X_test = test_data[available_features].values\n",
    "    \n",
    "    # Reshape for model input\n",
    "    X_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    \n",
    "    # Get predictions (probabilities)\n",
    "    predictions = model.predict(X_reshaped)\n",
    "    \n",
    "    # Convert to class labels (0, 1, 2)\n",
    "    # Assuming: 0=benign, 1=Neris/Rbot, 2=other attacks\n",
    "    predicted_labels = np.argmax(predictions, axis=1) if predictions.shape[1] > 1 else (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Filter samples where prediction is 2\n",
    "    mask_label_2 = predicted_labels == 2\n",
    "    layer2_attacks = test_data[mask_label_2].copy()\n",
    "    \n",
    "    print(f\"Layer 2 detected {len(layer2_attacks)} attacks with label 2\")\n",
    "    \n",
    "    return layer2_attacks, predicted_labels\n",
    "\n",
    "# Usage\n",
    "layer2_attacks_df, all_predictions = predict_layer2(layer2_model,attack_df_layer1, available_set_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "852a1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\anaconda-python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\anaconda-python\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement RandomForestClassifier (from versions: none)\n",
      "ERROR: No matching distribution found for RandomForestClassifier\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\anaconda-python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\anaconda-python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\anaconda-python\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3a6c341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Layer 3 RF on 38898 samples, 17 features\n",
      "Classes: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def train_layer3_rf(df_attack):\n",
    "    available_set_b = [f for f in set_b_features if f in df_attack.columns]\n",
    "    X = df_attack[available_set_b]\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df_attack['Label'])\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X, y)\n",
    "    \n",
    "    print(f\"Trained Layer 3 RF on {len(df_attack)} samples, {len(available_set_b)} features\")\n",
    "    print(f\"Classes: {le.classes_}\")\n",
    "    \n",
    "    return rf_model, le\n",
    "    \n",
    "    return rf_model, le\n",
    "rf_layer3,label_encoder  = train_layer3_rf(df_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24a73594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 predictions: (array([1], dtype=int64), array([38898], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "def predict_layer3(rf_model, label_encoder, layer2_output_df):\n",
    "    \"\"\"Predict specific botnet classes using Random Forest\"\"\"\n",
    "    # Select Set B features\n",
    "    X = layer2_output_df[set_b_features]\n",
    "    \n",
    "    # Predict classes\n",
    "    predictions = rf_model.predict(X)\n",
    "    predicted_classes = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    result_df = layer2_output_df.copy()\n",
    "    result_df['Predicted_Class'] = predicted_classes\n",
    "    result_df['Predicted_Label'] = predictions\n",
    "    \n",
    "    print(f\"Layer 3 predictions: {np.unique(predicted_classes, return_counts=True)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# Predict using Layer 3\n",
    "layer3_results = predict_layer3(rf_layer3, label_encoder, df_attack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8d2eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE for 1...\n",
      "Trained VAE for 1 on 38898 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda-python\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 17 features, but StandardScaler is expecting 59 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m vae_models_layer4, scalers_layer4 \u001b[38;5;241m=\u001b[39m build_four_vaes(df_attack)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Apply Layer 4 detection\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m layer4_results \u001b[38;5;241m=\u001b[39m \u001b[43mlayer4_zero_day_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer3_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae_models_layer4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers_layer4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer 4 Zero-day Detection Complete:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKnown attacks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer4_results[layer4_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_known_attack\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[54], line 52\u001b[0m, in \u001b[0;36mlayer4_zero_day_detection\u001b[1;34m(layer3_results, vae_models, scalers, threshold)\u001b[0m\n\u001b[0;32m     49\u001b[0m scaler \u001b[38;5;241m=\u001b[39m scalers[predicted_class]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m data_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Get reconstruction\u001b[39;00m\n\u001b[0;32m     55\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mpredict(data_scaled)\n",
      "File \u001b[1;32md:\\anaconda-python\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda-python\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32md:\\anaconda-python\\lib\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\anaconda-python\\lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 17 features, but StandardScaler is expecting 59 features as input."
     ]
    }
   ],
   "source": [
    "def build_four_vaes(df_attack):\n",
    "    \"\"\"Build 4 separate VAEs for each botnet class in Layer 4\"\"\"\n",
    "    # Get unique botnet classes from labels\n",
    "    unique_classes = df_attack['Label'].unique()\n",
    "    vae_models = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for botnet_class in unique_classes:\n",
    "        print(f\"Training VAE for {botnet_class}...\")\n",
    "        \n",
    "        # Filter data for this specific botnet class\n",
    "        class_data = df_attack[df_attack['Label'] == botnet_class]\n",
    "        \n",
    "        # Select numeric features\n",
    "        numeric_data = class_data.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if len(numeric_data) == 0:\n",
    "            print(f\"No numeric data for {botnet_class}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(numeric_data)\n",
    "        \n",
    "        # Build and train VAE\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        vae = build_vae_simple(input_dim)\n",
    "        vae.fit(X_scaled, X_scaled, epochs=30, batch_size=16, verbose=0)\n",
    "        \n",
    "        # Store model and scaler\n",
    "        vae_models[botnet_class] = vae\n",
    "        scalers[botnet_class] = scaler\n",
    "        \n",
    "        print(f\"Trained VAE for {botnet_class} on {len(class_data)} samples\")\n",
    "    \n",
    "    return vae_models, scalers\n",
    "\n",
    "def layer4_zero_day_detection(layer3_results, vae_models, scalers, threshold=0.1):\n",
    "    \"\"\"Layer 4: Check if predictions match known patterns using VAEs\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in layer3_results.iterrows():\n",
    "        predicted_class = row['Predicted_Class']\n",
    "        actual_data = row[set_b_features].values.reshape(1, -1)\n",
    "        \n",
    "        if predicted_class in vae_models:\n",
    "            # Get the VAE and scaler for this class\n",
    "            vae = vae_models[predicted_class]\n",
    "            scaler = scalers[predicted_class]\n",
    "            \n",
    "            # Scale the data\n",
    "            data_scaled = scaler.transform(actual_data)\n",
    "            \n",
    "            # Get reconstruction\n",
    "            reconstruction = vae.predict(data_scaled)\n",
    "            \n",
    "            # Calculate MAE\n",
    "            mae = np.mean(np.abs(data_scaled - reconstruction))\n",
    "            \n",
    "            # If MAE is low, it's a known attack; if high, it's zero-day\n",
    "            is_known_attack = mae <= threshold\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'predicted_class': predicted_class,\n",
    "                'mae': mae,\n",
    "                'is_known_attack': is_known_attack,\n",
    "                'is_zero_day': not is_known_attack\n",
    "            })\n",
    "        else:\n",
    "            # No VAE for this class - treat as zero-day\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'predicted_class': predicted_class,\n",
    "                'mae': None,\n",
    "                'is_known_attack': False,\n",
    "                'is_zero_day': True\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Build 4 VAEs for Layer 4\n",
    "vae_models_layer4, scalers_layer4 = build_four_vaes(df_attack)\n",
    "\n",
    "# Apply Layer 4 detection\n",
    "layer4_results = layer4_zero_day_detection(layer3_results, vae_models_layer4, scalers_layer4)\n",
    "\n",
    "print(\"Layer 4 Zero-day Detection Complete:\")\n",
    "print(f\"Known attacks: {len(layer4_results[layer4_results['is_known_attack']])}\")\n",
    "print(f\"Zero-day attacks: {len(layer4_results[layer4_results['is_zero_day']])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
